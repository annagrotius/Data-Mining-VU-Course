{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-94b6de02fc80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e1a1dda961b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Open csv as data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/Users/Marieke/Documents/Atom_Code/DMT/SmsCollection.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"am;\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'spam'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Open csv as data\n",
    "texts = pd.read_csv('/Users/Marieke/Documents/Atom_Code/DMT/SmsCollection.csv', sep = \"am;\", engine = 'python')\n",
    "texts.index.name = 'spam'\n",
    "texts.reset_index(inplace=True)\n",
    "texts.columns = ['spam', 'text']\n",
    "\n",
    "# Change labels to 1 (spam) and 0 (ham)\n",
    "for i in texts.index:\n",
    "    if re.match(r'sp', texts['spam'][i]):\n",
    "        texts.at[i, 'spam'] = 1\n",
    "    else:\n",
    "        texts.at[i, 'spam'] = 0\n",
    "\n",
    "totalTexts = texts['text'].shape[0]\n",
    "trainIndex, testIndex = list(), list()\n",
    "\n",
    "for i in range(totalTexts):\n",
    "    if np.random.uniform(0, 1) < 0.75:\n",
    "        trainIndex += [i]\n",
    "    else:\n",
    "        testIndex += [i]\n",
    "\n",
    "trainData = texts.loc[trainIndex]\n",
    "testData = texts.loc[testIndex]\n",
    "\n",
    "trainData.reset_index(inplace = True)\n",
    "trainData.drop(['index'], axis = 1, inplace = True)\n",
    "trainData.head()\n",
    "\n",
    "testData.reset_index(inplace = True)\n",
    "testData.drop(['index'], axis = 1, inplace = True)\n",
    "testData.head()\n",
    "\n",
    "# Look which words are most present in spam\n",
    "spam_words = ' '.join(list(texts[texts['spam'] == 1]['text']))\n",
    "spam_wc = WordCloud(width = 512, height = 512).generate(spam_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(spam_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "# Look which words are most present in ham\n",
    "ham_words = ' '.join(list(texts[texts['spam'] == 0]['text']))\n",
    "ham_wc = WordCloud(width = 512, height = 512).generate(ham_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(ham_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "# Tokenize, remove stopwords, lemmatize\n",
    "def process_message(text, lower_case = True, stem = True, stop_words = True, gram = 2):\n",
    "    if lower_case:\n",
    "        text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    if gram > 1:\n",
    "        w = []\n",
    "        for i in range(len(words) - gram + 1):\n",
    "            w += [' '.join(words[i:i + gram])]\n",
    "        return w\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "# Define SpamClassifier\n",
    "class SpamClassifier(object):\n",
    "    def _init_(self, trainData, method = 'tf-idf'):\n",
    "        self.texts, self.labels = trainData['text'], trainData['spam']\n",
    "        self.method = method\n",
    "\n",
    "    def train(self):\n",
    "        self.calc_TF_and_IDF()\n",
    "        if self.method == 'tf-idf':\n",
    "            self.calc_TF_IDF()\n",
    "        else:\n",
    "            self.calc_prob()\n",
    "\n",
    "    def calc_prob(self):\n",
    "        self.prob_spam = dict()\n",
    "        self.prob_ham = dict()\n",
    "        for word in self.tf_spam:\n",
    "            self.prob_spam[word] = (self.tf_spam[word] + 1) / (self.spam_words + \\\n",
    "                                                                len(list(self.tf_spam.keys())))\n",
    "        for word in self.tf_ham:\n",
    "            self.prob_ham[word] = (self.tf_ham[word] + 1) / (self.ham_words + \\\n",
    "                                                                len(list(self.tf_ham.keys())))\n",
    "        self.prob_spam_text, self.prob_ham_text = self.spam_texts / self.total_texts, self.ham_texts / self.total_texts\n",
    "\n",
    "    def calc_TF_and_IDF(self):\n",
    "        noOfMessages = self.texts.shape[0]\n",
    "        self.spam_texts, self.ham_texts = self.labels.value_counts()[1], self.labels.value_counts()[0]\n",
    "        self.total_texts = self.spam_texts + self.ham_texts\n",
    "        self.spam_words = 0\n",
    "        self.ham_words = 0\n",
    "        self.tf_spam = dict()\n",
    "        self.tf_ham = dict()\n",
    "        self.idf_spam = dict()\n",
    "        self.idf_ham = dict()\n",
    "        for i in range(noOfMessages):\n",
    "            message_processed = process_message(self.texts[i])\n",
    "            count = list() #To keep track of whether the word has ocured in the message or not.\n",
    "                           #For IDF\n",
    "            for word in message_processed: # for-loop to count all instances of a word, and add it to spam or ham list\n",
    "                if self.labels[i]:\n",
    "                    self.tf_spam[word] = self.tf_spam.get(word, 0) + 1\n",
    "                    self.spam_words += 1\n",
    "                else:\n",
    "                    self.tf_ham[word] = self.tf_ham.get(word, 0) + 1\n",
    "                    self.ham_words += 1\n",
    "                if word not in count:\n",
    "                    count += [word]\n",
    "            for word in count:\n",
    "                if self.labels[i]:\n",
    "                    self.idf_spam[word] = self.idf_spam.get(word, 0) + 1\n",
    "                else:\n",
    "                    self.idf_ham[word] = self.idf_ham.get(word, 0) + 1\n",
    "\n",
    "    def calc_TF_IDF(self):\n",
    "        self.prob_spam = dict()\n",
    "        self.prob_ham = dict()\n",
    "        self.sum_tf_idf_spam = 0\n",
    "        self.sum_tf_idf_ham = 0\n",
    "        for word in self.tf_spam:\n",
    "            self.prob_spam[word] = (self.tf_spam[word]) * log((self.spam_texts + self.ham_texts) \\\n",
    "                                                          / (self.idf_spam[word] + self.idf_ham.get(word, 0)))\n",
    "            self.sum_tf_idf_spam += self.prob_spam[word]\n",
    "        for word in self.tf_spam:\n",
    "            self.prob_spam[word] = (self.prob_spam[word] + 1) / (self.sum_tf_idf_spam + len(list(self.prob_spam.keys())))\n",
    "\n",
    "        for word in self.tf_ham:\n",
    "            self.prob_ham[word] = (self.tf_ham[word]) * log((self.spam_texts + self.ham_texts) \\\n",
    "                                                          / (self.idf_spam.get(word, 0) + self.idf_ham[word]))\n",
    "            self.sum_tf_idf_ham += self.prob_ham[word]\n",
    "        for word in self.tf_ham:\n",
    "            self.prob_ham[word] = (self.prob_ham[word] + 1) / (self.sum_tf_idf_ham + len(list(self.prob_ham.keys())))\n",
    "\n",
    "\n",
    "        self.prob_spam_text, self.prob_ham_text = self.spam_texts / self.total_texts, self.ham_texts / self.total_texts\n",
    "\n",
    "    def classify(self, processed_message):\n",
    "        pSpam, pHam = 0, 0\n",
    "        for word in processed_message:\n",
    "            if word in self.prob_spam:\n",
    "                pSpam += log(self.prob_spam[word])\n",
    "            else:\n",
    "                if self.method == 'tf-idf':\n",
    "                    pSpam -= log(self.sum_tf_idf_spam + len(list(self.prob_spam.keys())))\n",
    "                else:\n",
    "                    pSpam -= log(self.spam_words + len(list(self.prob_spam.keys())))\n",
    "            if word in self.prob_ham:\n",
    "                pHam += log(self.prob_ham[word])\n",
    "            else:\n",
    "                if self.method == 'tf-idf':\n",
    "                    pHam -= log(self.sum_tf_idf_ham + len(list(self.prob_ham.keys())))\n",
    "                else:\n",
    "                    pHam -= log(self.ham_words + len(list(self.prob_ham.keys())))\n",
    "            pSpam += log(self.prob_spam_text)\n",
    "            pHam += log(self.prob_ham_text)\n",
    "        return pSpam >= pHam\n",
    "\n",
    "    def predict(self, testData):\n",
    "        result = dict()\n",
    "        for (i, message) in enumerate(testData):\n",
    "            processed_message = process_message(message)\n",
    "            result[i] = int(self.classify(processed_message))\n",
    "        return result\n",
    "\n",
    "def metrics(labels, predictions):\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "    for i in range(len(labels)):\n",
    "        true_pos += int(labels[i] == 1 and predictions[i] == 1)\n",
    "        true_neg += int(labels[i] == 0 and predictions[i] == 0)\n",
    "        false_pos += int(labels[i] == 0 and predictions[i] == 1)\n",
    "        false_neg += int(labels[i] == 1 and predictions[i] == 0)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    Fscore = 2 * precision * recall / (precision + recall)\n",
    "    accuracy = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F-score: \", Fscore)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "sc_tf_idf = SpamClassifier(trainData, 'tf-idf')\n",
    "sc_tf_idf.train()\n",
    "preds_tf_idf = sc_tf_idf.predict(testData['text'])\n",
    "metrics(testData['spam'], preds_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
